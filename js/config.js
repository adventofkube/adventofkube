// Set the start date for day unlocking.
// Day 1 unlocks on START_DATE, Day 2 on START_DATE + 1 day, etc.
export const START_DATE = new Date('2026-01-05T00:00:00');

// Per-day configuration. flagHash is the SHA-256 hex digest of the correct flag.
export const DAYS = [
  {
    day: 0,
    enabled: true,
    title: 'Setup',
    description: 'Set up your local environment, deploy a test chart, and retrieve the flag from the pod logs.',
    flagHash: '1c015e563e5e30188378f8516b89cfa5c6c14ba00f9efd5a878a570e383e2e03',
    chartUrl: 'https://example.com/charts/day00.tgz',
    osSetup: {
      windows: [
        { title: '1. Install Docker Desktop', content: 'Download and install <a href="https://docs.docker.com/desktop/install/windows-install/" target="_blank" rel="noopener">Docker Desktop</a>. Enable WSL2 when prompted.' },
        { title: '2. Install kubectl', content: 'Download <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/" target="_blank" rel="noopener">kubectl.exe</a> and add it to your PATH, or run in PowerShell:<code>winget install Kubernetes.kubectl</code>' },
        { title: '3. Install kind', content: 'Download from <a href="https://kind.sigs.k8s.io/docs/user/quick-start/#installing-from-release-binaries" target="_blank" rel="noopener">kind releases</a> and add to PATH, or run:<code>winget install Kubernetes.kind</code>' },
        { title: '4. Install Helm', content: 'Download from <a href="https://helm.sh/docs/intro/install/#from-the-binary-releases" target="_blank" rel="noopener">Helm releases</a> and add to PATH, or run:<code>winget install Helm.Helm</code>' },
      ],
      mac: [
        { title: '1. Install Docker Desktop', content: '<code>brew install --cask docker</code> or download from <a href="https://docs.docker.com/desktop/install/mac-install/" target="_blank" rel="noopener">Docker Desktop</a>' },
        { title: '2. Install kubectl', content: '<code>brew install kubectl</code>' },
        { title: '3. Install kind', content: '<code>brew install kind</code>' },
        { title: '4. Install Helm', content: '<code>brew install helm</code>' },
      ],
      linux: [
        { title: '1. Install Docker Engine', content: 'Follow the <a href="https://docs.docker.com/engine/install/" target="_blank" rel="noopener">Docker Engine install guide</a> for your distro, then run <code>sudo systemctl start docker</code>' },
        { title: '2. Install kubectl', content: 'Follow the <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/" target="_blank" rel="noopener">kubectl install guide</a>' },
        { title: '3. Install kind', content: 'Follow the <a href="https://kind.sigs.k8s.io/docs/user/quick-start/#installation" target="_blank" rel="noopener">kind install guide</a>' },
        { title: '4. Install Helm', content: 'Follow the <a href="https://helm.sh/docs/intro/install/" target="_blank" rel="noopener">Helm install guide</a>' },
      ],
    },
    setup: [
      '<strong>5. Verify installations</strong>',
      '<code>docker --version\nkubectl version --client\nkind --version\nhelm version</code>',
      '<strong>6. Create a kind cluster</strong>',
      '<code>kind create cluster --name adventofkube</code>',
      '<strong>7. Install the Day 0 chart</strong>',
      '<code>helm install day00 oci://ghcr.io/adventofkube/charts/day00 --version 0.1.0</code>',
      '<strong>8. Get the flag from the pod logs</strong>',
      '<code>kubectl logs -n day00 setup-check</code>',
    ],
    hints: [
      'Make sure Docker is running before creating the kind cluster.',
      'If <code>kind create cluster</code> fails, try <code>docker ps</code> to check that Docker is responsive.',
      'On Linux, you may need to run Docker commands with <code>sudo</code> or add your user to the <code>docker</code> group.',
      'If the pod isn\'t showing logs yet, run <code>kubectl get pods -n day00</code> and wait for it to complete.',
    ],
    docs: [
      { title: 'Install kubectl', url: 'https://kubernetes.io/docs/tasks/tools/' },
      { title: 'kind Quick Start', url: 'https://kind.sigs.k8s.io/docs/user/quick-start/' },
      { title: 'Install Helm', url: 'https://helm.sh/docs/intro/install/' },
      { title: 'kubectl Cheat Sheet', url: 'https://kubernetes.io/docs/reference/kubectl/cheatsheet/' },
    ],
  },
  {
    day: 1,
    enabled: true,
    title: 'Pods',
    description: 'A pod won\'t start. Investigate the issue, fix it, and retrieve the flag from the logs.',
    flagHash: 'd88ee18014da63147665ad1455bada2141eb3ba3bbb11051053a74ab2e3aa5c7',
    chartUrl: 'https://example.com/charts/day01.tgz',
    setup: [
      'Install the chart:',
      '<code>helm install day01 oci://ghcr.io/adventofkube/charts/day01 --version 0.2.0</code>',
      'Start investigating:',
      '<code>kubectl get pods -n day01</code>',
      'Once the pod is running, get the flag:',
      '<code>kubectl logs -n day01 day01-pod</code>',
    ],
    hints: [
      'Check the pod status and events:<code>kubectl get pods -n day01\nkubectl describe pod day01-pod -n day01</code>Look at the Events section at the bottom.',
      'The Events mention a failed image pull. Look closely at the image name — does it look right?',
      'The image name is <code>daay01</code> (with two a\'s). It should be <code>day01</code>. Delete the pod and recreate it with the correct image:<code>kubectl get pod day01-pod -n day01 -o yaml > fix.yaml\n# Edit fix.yaml: change daay01 to day01\nkubectl delete pod day01-pod -n day01\nkubectl apply -f fix.yaml</code>',
    ],
    docs: [
      { title: 'kubectl get', url: 'https://kubernetes.io/docs/reference/kubectl/generated/kubectl_get/' },
      { title: 'kubectl describe', url: 'https://kubernetes.io/docs/reference/kubectl/generated/kubectl_describe/' },
      { title: 'kubectl logs', url: 'https://kubernetes.io/docs/reference/kubectl/generated/kubectl_logs/' },
      { title: 'Debugging Pods', url: 'https://kubernetes.io/docs/tasks/debug/debug-application/debug-pods/' },
    ],
  },
  {
    day: 2,
    enabled: true,
    title: 'ConfigMaps',
    description: 'A pod is stuck and won\'t start. Something is wrong with its configuration. Find and fix the issue.',
    flagHash: '5b43f80414de6df30a6ae54f681052e5c5e35cb9cfa57cc24f169422fb315003',
    chartUrl: 'https://example.com/charts/day02.tgz',
    setup: [
      'Install the chart:',
      '<code>helm install day02 oci://ghcr.io/adventofkube/charts/day02 --version 0.2.0</code>',
      'Check the pod status:',
      '<code>kubectl get pods -n day02</code>',
      'The pod is stuck. Investigate why:',
      '<code>kubectl describe pod day02-pod -n day02</code>',
      'Once the pod is running, get the flag:',
      '<code>kubectl logs -n day02 day02-pod</code>',
    ],
    hints: [
      'The pod status shows CreateContainerConfigError. Look at the Events section in <code>kubectl describe</code> — what does the error message say?',
      'The error mentions a ConfigMap that doesn\'t exist. The pod expects <code>envFrom</code> to load from a ConfigMap. Check which one:<code>kubectl get pod day02-pod -n day02 -o yaml | grep configMapRef -A 1</code>',
      'The pod references <code>day02-settings</code>, but the actual ConfigMap is named <code>day02-config</code>:<code>kubectl get configmap -n day02</code>Export the pod, fix the reference, delete and recreate:<code>kubectl get pod day02-pod -n day02 -o yaml > fix.yaml\n# Edit fix.yaml: change day02-settings to day02-config\nkubectl delete pod day02-pod -n day02\nkubectl apply -f fix.yaml</code>',
    ],
    docs: [
      { title: 'kubectl logs', url: 'https://kubernetes.io/docs/reference/kubectl/generated/kubectl_logs/' },
      { title: 'Debugging Pods', url: 'https://kubernetes.io/docs/tasks/debug/debug-application/debug-pods/' },
      { title: 'ConfigMaps', url: 'https://kubernetes.io/docs/concepts/configuration/configmap/' },
      { title: 'Define Environment Variables Using a ConfigMap', url: 'https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/' },
    ],
  },
  {
    day: 3,
    enabled: true,
    title: 'Services',
    description: 'A Service can\'t reach its backend. The pod is running, but something is wrong. Debug the connectivity.',
    flagHash: 'bc64f86e6a230e279ff0a5e095a48e52d1e234525b748e64f27c1680913cdf12',
    chartUrl: 'https://example.com/charts/day03.tgz',
    setup: [
      'Install the chart:',
      '<code>helm install day03 oci://ghcr.io/adventofkube/charts/day03 --version 0.3.0</code>',
      'Check the pod is running:',
      '<code>kubectl get pods -n day03</code>',
      'Try to reach the service (this will fail initially):',
      '<code>kubectl run curl --rm -it --image=curlimages/curl --restart=Never -n day03 -- curl -m 5 http://flag-server</code>',
      'The <code>-m 5</code> sets a 5-second timeout. Once the service is fixed, it will return the flag.',
    ],
    hints: [
      'The pod is running, but the Service can\'t reach it. Check the Service endpoints:<code>kubectl get endpoints -n day03</code>If it shows no endpoints, the Service isn\'t finding any pods.',
      'Services find pods using label selectors. Does this Service have a selector?<code>kubectl get svc flag-server -n day03 -o yaml</code>Check what labels the Pod has:<code>kubectl get pods -n day03 --show-labels</code>',
      'The Service has no selector! You can fix this by editing the Service to add one:<code>kubectl edit svc flag-server -n day03</code>Or delete it and use <code>kubectl expose</code>:<code>kubectl delete svc flag-server -n day03</code><code>kubectl expose deployment flag-server -n day03 --port=80 --target-port=8080</code>',
    ],
    docs: [
      { title: 'Services', url: 'https://kubernetes.io/docs/concepts/services-networking/service/' },
      { title: 'Labels and Selectors', url: 'https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/' },
      { title: 'kubectl expose', url: 'https://kubernetes.io/docs/reference/kubectl/generated/kubectl_expose/' },
      { title: 'Debugging Services', url: 'https://kubernetes.io/docs/tasks/debug/debug-application/debug-service/' },
    ],
  },
  {
    day: 4,
    enabled: true,
    title: 'Secrets',
    description: 'A pod won\'t start. It needs a Secret, but something isn\'t configured correctly. Figure out what\'s wrong.',
    flagHash: 'f42b4a89a4b7f81ddd3d30e30746e6d6fca8a5819c7dd3c4309a0c4adf0b310d',
    chartUrl: 'https://example.com/charts/day04.tgz',
    setup: [
      'Install the chart:',
      '<code>helm install day04 oci://ghcr.io/adventofkube/charts/day04 --version 0.1.0</code>',
      'Check the pod status:',
      '<code>kubectl get pods -n day04</code>',
      'The pod is stuck. Investigate why:',
      '<code>kubectl describe pod secret-app -n day04</code>',
      'Once the pod runs successfully, get the flag:',
      '<code>kubectl logs -n day04 secret-app</code>',
    ],
    hints: [
      'The pod status shows CreateContainerConfigError. The Events section in <code>kubectl describe</code> shows an error about a missing Secret key.',
      'The error says key <code>API_KEY</code> not found in Secret. Check what keys the Secret actually has:<code>kubectl get secret app-credentials -n day04 -o jsonpath=\'{.data}\'</code>You\'ll see the key names (the values are base64 encoded, but key names are visible).',
      'The Secret has key <code>api-key</code> but the Pod expects <code>API_KEY</code>. Fix the Secret:<code>kubectl get secret app-credentials -n day04 -o yaml > fix.yaml\n# Edit fix.yaml: rename the key from api-key to API_KEY\nkubectl apply -f fix.yaml\nkubectl delete pod secret-app -n day04</code>The pod will be recreated automatically.',
    ],
    docs: [
      { title: 'Secrets', url: 'https://kubernetes.io/docs/concepts/configuration/secret/' },
      { title: 'Using Secrets as Environment Variables', url: 'https://kubernetes.io/docs/concepts/configuration/secret/#using-secrets-as-environment-variables' },
      { title: 'kubectl describe', url: 'https://kubernetes.io/docs/reference/kubectl/generated/kubectl_describe/' },
    ],
  },
  {
    day: 5,
    enabled: true,
    title: 'Resource Quotas',
    description: 'A Deployment exists but no pods are being created. Something is preventing the pods from being scheduled.',
    flagHash: 'db755eb20ac5651ce8080506b9dab20b81b1af45a52112e67cd13a2c755b0a87',
    chartUrl: 'https://example.com/charts/day05.tgz',
    setup: [
      'Install the chart:',
      '<code>helm install day05 oci://ghcr.io/adventofkube/charts/day05 --version 0.1.0</code>',
      'Check the deployment:',
      '<code>kubectl get deployments -n day05</code>',
      'Notice the deployment has 0 pods ready. Investigate why:',
      '<code>kubectl describe deployment quota-app -n day05</code>',
      'Once a pod is running, get the flag:',
      '<code>kubectl logs -n day05 -l app=quota-app</code>',
    ],
    hints: [
      'The Deployment shows 0/1 replicas ready, but there are no pods. Check the ReplicaSet events:<code>kubectl get rs -n day05\nkubectl describe rs -n day05</code>Look at the Events section for errors.',
      'The error mentions "exceeded quota". Check the namespace\'s ResourceQuota:<code>kubectl get resourcequota -n day05\nkubectl describe resourcequota day05-quota -n day05</code>Compare the quota limits to what the Deployment requests.',
      'The quota allows 200m CPU / 128Mi memory for requests, but the Deployment requests 500m / 256Mi. Edit the Deployment to reduce resources:<code>kubectl edit deployment quota-app -n day05</code>Set requests to cpu: 100m, memory: 64Mi (and matching limits within quota).',
    ],
    docs: [
      { title: 'Resource Quotas', url: 'https://kubernetes.io/docs/concepts/policy/resource-quotas/' },
      { title: 'Managing Resources for Containers', url: 'https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/' },
      { title: 'kubectl describe', url: 'https://kubernetes.io/docs/reference/kubectl/generated/kubectl_describe/' },
    ],
  },
  {
    day: 6,
    enabled: true,
    title: 'Deployments',
    description: 'A Deployment is failing to roll out. There are 3 things wrong. Diagnose each issue and fix them to get the flag.',
    flagHash: '3d9822e477d0414cc8c153847fbb667394143b9301c816e2b9eb0efb8bb737e4',
    chartUrl: 'https://example.com/charts/day06.tgz',
    setup: [
      'Install the chart:',
      '<code>helm install day06 oci://ghcr.io/adventofkube/charts/day06 --version 0.2.0</code>',
      'Start investigating:',
      '<code>kubectl get pods -n day06</code>',
      'Once all issues are fixed, curl the service for the flag:',
      '<code>kubectl run curl --rm -it --image=curlimages/curl --restart=Never -n day06 -- curl -m 5 http://day06</code>',
    ],
    hints: [
      'Check the pod status and events:<code>kubectl describe pod -n day06 -l app=day06</code>Look at the Events section — what does it say about the image?',
      'The image tag doesn\'t exist in the registry. Edit the Deployment to use a valid tag:<code>kubectl edit deployment day06 -n day06</code>',
      'After fixing the image, the pod crashes. Check the logs:<code>kubectl logs -n day06 -l app=day06</code>The error message tells you exactly what\'s missing.',
      'The app needs an environment variable. There\'s a ConfigMap in the namespace:<code>kubectl get configmap -n day06 -o yaml</code>Add an <code>envFrom</code> block to the Deployment referencing the ConfigMap.',
      'The pod is running but the Service can\'t reach it. Check the Service ports:<code>kubectl get svc day06 -n day06 -o yaml</code>The app listens on <code>8080</code>. Fix the <code>targetPort</code>.',
    ],
    docs: [
      { title: 'Deployments', url: 'https://kubernetes.io/docs/concepts/workloads/controllers/deployment/' },
      { title: 'ConfigMaps as Env Vars', url: 'https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/#configure-all-key-value-pairs-in-a-configmap-as-container-environment-variables' },
      { title: 'Service Ports', url: 'https://kubernetes.io/docs/concepts/services-networking/service/#defining-a-service' },
      { title: 'kubectl describe', url: 'https://kubernetes.io/docs/reference/kubectl/generated/kubectl_describe/' },
    ],
  },
  {
    day: 7,
    enabled: true,
    title: 'Service Wiring',
    description: 'A Service can\'t route to its pod. The deployment is running but the Service port configuration is wrong. Fix the wiring.',
    flagHash: '3d3d1fb274ca5d09414bb2b2f9fcf8a663de10cdaffdd880e91980a3d50b817a',
    setup: [
      'Install the chart:',
      '<code>helm install day07 oci://ghcr.io/adventofkube/charts/day07 --version 0.1.0</code>',
      'Test the service:',
      '<code>kubectl run curl --rm -it --image=curlimages/curl --restart=Never -n day07 -- curl -m 5 http://flag-server</code>',
    ],
    hints: [
      'The Service exists and the pod is running. Check if the Service has endpoints:<code>kubectl get endpoints -n day07</code>',
      'The endpoints exist but the Service can\'t connect. Compare the Service targetPort with the container port name:<code>kubectl get svc flag-server -n day07 -o yaml\nkubectl get pods -n day07 -o yaml | grep -A2 ports</code>',
      'The Service targetPort is "https" but the container port is named "web". Change the targetPort to "web" or to 8080.',
    ],
    docs: [
      { title: 'Services', url: 'https://kubernetes.io/docs/concepts/services-networking/service/' },
      { title: 'Debugging Services', url: 'https://kubernetes.io/docs/tasks/debug/debug-application/debug-service/' },
    ],
  },
  {
    day: 8,
    enabled: true,
    title: 'Probe Pitfall',
    description: 'A pod keeps restarting and is never ready. The health checks are misconfigured in multiple ways. Fix them all.',
    flagHash: 'f3488d3f7b71217101c6d22e81016d4c1fabdacb9b4d7dc4b860656b41dbbbfe',
    setup: [
      'Install the chart:',
      '<code>helm install day08 oci://ghcr.io/adventofkube/charts/day08 --version 0.1.0</code>',
      'Watch the pod status:',
      '<code>kubectl get pods -n day08 -w</code>',
      'Once the pod is stable, curl the service:',
      '<code>kubectl run curl --rm -it --image=curlimages/curl --restart=Never -n day08 -- curl -m 5 http://probe-app</code>',
    ],
    hints: [
      'The pod keeps restarting. Check events:<code>kubectl describe pod -n day08 -l app=probe-app</code>The liveness probe is failing — what path does it check?',
      'The liveness probe checks /healthz but the app only serves /health. Fix the liveness path. But there are more issues...',
      'The readiness probe fires immediately (0s delay) but the app takes 3 seconds to start. Also, periodSeconds=1 and failureThreshold=1 are too aggressive. Fix all three: liveness path, readiness initialDelay, and readiness threshold.',
    ],
    docs: [
      { title: 'Configure Liveness & Readiness Probes', url: 'https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/' },
      { title: 'Pod Lifecycle', url: 'https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/' },
    ],
  },
  {
    day: 9,
    enabled: true,
    title: 'RBAC Lockdown',
    description: 'A pod needs to list Secrets but its ServiceAccount doesn\'t have the right permissions. Fix the RBAC configuration.',
    flagHash: '8cdbc41767786542284d8cb18a5c031ac77e640b1af6dfec3d907dd63d299e47',
    setup: [
      'Install the chart:',
      '<code>helm install day09 oci://ghcr.io/adventofkube/charts/day09 --version 0.1.0</code>',
      'Check the pod logs:',
      '<code>kubectl logs -n day09 rbac-app</code>',
    ],
    hints: [
      'The pod logs show "Cannot list secrets". Check the Role:<code>kubectl get role -n day09 -o yaml</code>What verbs are allowed?',
      'The Role only has "get" but the app calls List(). Add "list" to the Role verbs.',
      'Edit the Role:<code>kubectl edit role secret-reader -n day09</code>Add "list" to the verbs array, then delete the pod to restart it.',
    ],
    docs: [
      { title: 'RBAC Authorization', url: 'https://kubernetes.io/docs/reference/access-authn-authz/rbac/' },
      { title: 'Using RBAC', url: 'https://kubernetes.io/docs/reference/access-authn-authz/rbac/#role-and-clusterrole' },
    ],
  },
  {
    day: 10,
    enabled: true,
    title: 'NetworkPolicy Firewall',
    description: 'A pod has the flag but a NetworkPolicy is blocking all traffic. Fix the policy to allow access.',
    flagHash: '83ef97ed32bdb69b8d2f89797a61f6033a5b9688f320be047c569f6773973d70',
    setup: [
      'Install the chart:',
      '<code>helm install day10 oci://ghcr.io/adventofkube/charts/day10 --version 0.1.0</code>',
      'Try to reach the service:',
      '<code>kubectl run curl --rm -it --image=curlimages/curl --restart=Never -n day10 -- curl -m 5 http://flag-server</code>',
    ],
    hints: [
      'The curl times out. Check if there\'s a NetworkPolicy:<code>kubectl get networkpolicy -n day10 -o yaml</code>',
      'There\'s a default-deny policy blocking all ingress. You need to add an ingress allow rule.',
      'Either delete the deny policy or create a new one that allows ingress on port 8080:<code>kubectl delete networkpolicy default-deny -n day10</code>',
    ],
    docs: [
      { title: 'Network Policies', url: 'https://kubernetes.io/docs/concepts/services-networking/network-policies/' },
      { title: 'Declare Network Policy', url: 'https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/' },
    ],
  },
  {
    day: 11,
    enabled: true,
    title: 'PV/PVC Binding',
    description: 'A PersistentVolumeClaim won\'t bind to its PersistentVolume. The pod is stuck waiting for storage. Fix the binding.',
    flagHash: '2c68f141f69bcd0fcbf0234b732caaf2694ebba03fc7497c8756e5f40ae356f9',
    setup: [
      'Install the chart:',
      '<code>helm install day11 oci://ghcr.io/adventofkube/charts/day11 --version 0.1.0</code>',
      'Check the PVC status:',
      '<code>kubectl get pvc -n day11</code>',
      'Once the pod runs, check its logs:',
      '<code>kubectl logs -n day11 storage-app</code>',
    ],
    hints: [
      'The PVC is Pending. Compare the PVC and PV:<code>kubectl get pv\nkubectl get pvc -n day11 -o yaml</code>Do the storageClassNames match?',
      'The PVC requests storageClassName "standard" but the PV uses "manual". Also check the requested size.',
      'Fix both: change PVC storageClassName to "manual" and reduce request from 2Gi to 1Gi (or less) to match the PV capacity.',
    ],
    docs: [
      { title: 'Persistent Volumes', url: 'https://kubernetes.io/docs/concepts/storage/persistent-volumes/' },
      { title: 'Configure a Pod to Use a PV', url: 'https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/' },
    ],
  },
  {
    day: 12,
    enabled: true,
    title: 'Rolling Update Stuck',
    description: 'A Deployment rollout is deadlocked. Fix the strategy, then update the image to v2 to get the flag.',
    flagHash: 'bbdb1db158f38fdc413c31ab6a98db402954270f7b251ea8e04d870ccf62ddcd',
    setup: [
      'Install the chart:',
      '<code>helm install day12 oci://ghcr.io/adventofkube/charts/day12 --version 0.1.0</code>',
      'Check the current version:',
      '<code>kubectl run curl --rm -it --image=curlimages/curl --restart=Never -n day12 -- curl -m 5 http://rollout-app</code>',
      'You need to update to v2 to get the flag.',
    ],
    hints: [
      'Try updating the image:<code>kubectl set image deployment/rollout-app rollout-app=ghcr.io/adventofkube/day12:v2 -n day12</code>Then watch the rollout:<code>kubectl rollout status deployment/rollout-app -n day12</code>It gets stuck. Why?',
      'Check the deployment strategy:<code>kubectl get deployment rollout-app -n day12 -o yaml | grep -A3 strategy</code>Both maxSurge and maxUnavailable are 0 — deadlock.',
      'Fix the strategy first:<code>kubectl patch deployment rollout-app -n day12 -p \'{"spec":{"strategy":{"rollingUpdate":{"maxSurge":1}}}}\'</code>Then the rollout to v2 will proceed.',
    ],
    docs: [
      { title: 'Deployments - Rolling Update', url: 'https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#rolling-update-deployment' },
      { title: 'kubectl rollout', url: 'https://kubernetes.io/docs/reference/kubectl/generated/kubectl_rollout/' },
    ],
  },
  {
    day: 13,
    enabled: true,
    title: 'Metrics Missing',
    description: 'Prometheus isn\'t scraping your app\'s metrics. The ServiceMonitor configuration has a mismatch. Fix it to see the flag in the metrics.',
    flagHash: 'd839e0669baea744e1af3debc51cd808d0138c866552628eda56122c4837acf3',
    setup: [
      'Install Prometheus (if not already):',
      '<code>helm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm install prometheus prometheus-community/kube-prometheus-stack -n monitoring --create-namespace</code>',
      'Install the chart:',
      '<code>helm install day13 oci://ghcr.io/adventofkube/charts/day13 --version 0.1.0</code>',
      'Check the app\'s /metrics endpoint directly:',
      '<code>kubectl run curl --rm -it --image=curlimages/curl --restart=Never -n day13 -- curl -m 5 http://metrics-app/metrics</code>',
    ],
    hints: [
      'The metrics endpoint works directly but Prometheus isn\'t scraping it. Check the ServiceMonitor:<code>kubectl get servicemonitor -n day13 -o yaml</code>Compare its matchLabels with the Service labels.',
      'The ServiceMonitor selects "metrics-server" but the Service label is "metrics-app". Fix the ServiceMonitor selector.',
      'The flag is in the HELP text of the flag_value metric. Once Prometheus scrapes it, query: <code>flag_value</code> and look at the HELP description.',
    ],
    docs: [
      { title: 'ServiceMonitor', url: 'https://prometheus-operator.dev/docs/user-guides/getting-started/#deploying-a-sample-application' },
      { title: 'Prometheus Operator', url: 'https://prometheus-operator.dev/docs/prologue/introduction/' },
    ],
  },
  {
    day: 14,
    enabled: true,
    title: 'Dashboard Down',
    description: 'A Grafana dashboard shows "No Data" because the datasource URL is wrong. Fix the configuration to see the flag.',
    flagHash: 'ad8aa6c10a82462066e4955b63d68b9e43fbb2b17210c92a50a5c41bd188251b',
    setup: [
      'Ensure Prometheus is installed (see Day 13).',
      'Install the chart:',
      '<code>helm install day14 oci://ghcr.io/adventofkube/charts/day14 --version 0.1.0</code>',
      'Port-forward Grafana:',
      '<code>kubectl port-forward svc/prometheus-grafana -n monitoring 3000:80</code>',
    ],
    hints: [
      'Open Grafana at localhost:3000. Find the "Advent of Kube - Day 14" dashboard. It shows "No Data".',
      'Check the datasource ConfigMap:<code>kubectl get configmap grafana-datasource -n day14 -o yaml</code>The Prometheus URL is wrong.',
      'Fix the datasource URL to match the actual Prometheus service name in your cluster (e.g., prometheus-kube-prometheus-prometheus.monitoring.svc:9090). The flag is in the dashboard panel title.',
    ],
    docs: [
      { title: 'Grafana Datasources', url: 'https://grafana.com/docs/grafana/latest/datasources/' },
      { title: 'Grafana Dashboard Provisioning', url: 'https://grafana.com/docs/grafana/latest/administration/provisioning/#dashboards' },
    ],
  },
  {
    day: 15,
    enabled: true,
    title: 'Alert Routing',
    description: 'An AlertManager alert fires correctly but the notification never reaches the webhook. The routing rules are misconfigured.',
    flagHash: '954ade5782c9db31550644dc20b42df2c28a4fe81823204641ba57bbdedea12b',
    setup: [
      'Ensure Prometheus + AlertManager are installed (see Day 13).',
      'Install the chart:',
      '<code>helm install day15 oci://ghcr.io/adventofkube/charts/day15 --version 0.1.0</code>',
      'Verify the alert fires:',
      '<code>kubectl port-forward svc/prometheus-kube-prometheus-alertmanager -n monitoring 9093:9093</code>',
    ],
    hints: [
      'The FlagReady alert fires with label team=infra. Check the AlertManager routing config:<code>kubectl get secret alertmanager-config -n day15 -o jsonpath=\'{.data.alertmanager\\.yaml}\' | base64 -d</code>',
      'The route matches team=backend but the alert has team=infra. The alert falls through to the null receiver.',
      'Fix the route match label from "backend" to "infra" so alerts route to the flag-webhook receiver.',
    ],
    docs: [
      { title: 'AlertManager Routing', url: 'https://prometheus.io/docs/alerting/latest/configuration/#route' },
      { title: 'PrometheusRule', url: 'https://prometheus-operator.dev/docs/user-guides/alerting/' },
    ],
  },
  {
    day: 16,
    enabled: true,
    title: 'Logs Lost',
    description: 'Promtail is supposed to collect logs but the scrape path glob is wrong. Fix it to find the flag in the logs.',
    flagHash: '081e574ceacd017e573eaec933617a78f2c0173a372ea550487421b7b71c5eb0',
    setup: [
      'Install Loki + Promtail:',
      '<code>helm repo add grafana https://grafana.github.io/helm-charts\nhelm install loki grafana/loki-stack -n monitoring --set promtail.enabled=true</code>',
      'Install the chart:',
      '<code>helm install day16 oci://ghcr.io/adventofkube/charts/day16 --version 0.1.0</code>',
    ],
    hints: [
      'The app prints the flag to stdout on startup. Check Promtail config:<code>kubectl get configmap promtail-config -n day16 -o yaml</code>Look at the __path__ glob.',
      'The scrape path is /var/log/pods/*/*/*.log but it should be /var/log/pods/*/*/*/*.log (4 levels, not 3).',
      'The Kubernetes log path format is: /var/log/pods/<namespace>_<pod>_<uid>/<container>/<restart_count>.log — that\'s 4 wildcards needed.',
    ],
    docs: [
      { title: 'Promtail Configuration', url: 'https://grafana.com/docs/loki/latest/send-data/promtail/configuration/' },
      { title: 'Kubernetes Logging', url: 'https://kubernetes.io/docs/concepts/cluster-administration/logging/' },
    ],
  },
  {
    day: 17,
    enabled: true,
    title: 'Pipeline Mismatch',
    description: 'Promtail\'s log parsing pipeline expects a different format than the app produces. Fix the pipeline to extract labels and find the flag.',
    flagHash: '781b090a4553f32ffd039bb8543bd078e89a24755e44654c989b21710a50511a',
    setup: [
      'Ensure Loki + Promtail are installed (see Day 16).',
      'Install the chart:',
      '<code>helm install day17 oci://ghcr.io/adventofkube/charts/day17 --version 0.1.0</code>',
      'Query logs in Grafana Explore with LogQL:',
      '<code>{job="pipeline-app"}</code>',
    ],
    hints: [
      'The Promtail pipeline uses a regex stage expecting "level=INFO msg=..." format. Check what the app actually logs:<code>kubectl logs -n day17 -l app=pipeline-app</code>',
      'The app logs JSON: {"level":"info","msg":"..."}. The regex doesn\'t match JSON format, so labels are never extracted.',
      'Replace the regex stage with a json stage in the Promtail config, or fix the regex to match JSON. The flag is in one of the log messages.',
    ],
    docs: [
      { title: 'Promtail Pipelines', url: 'https://grafana.com/docs/loki/latest/send-data/promtail/pipelines/' },
      { title: 'Pipeline Stages', url: 'https://grafana.com/docs/loki/latest/send-data/promtail/stages/' },
    ],
  },
  {
    day: 18,
    enabled: true,
    title: 'Node Affinity',
    description: 'A pod is stuck Pending because its node affinity requires a label that no node has. Fix the scheduling constraint.',
    flagHash: '1929f40692ebcb36dc53e37adfe10f3959c7bfad1d3c0f152d7b4b40d13ebb7b',
    setup: [
      'Install the chart:',
      '<code>helm install day18 oci://ghcr.io/adventofkube/charts/day18 --version 0.1.0</code>',
      'Check the pod status:',
      '<code>kubectl get pods -n day18</code>',
    ],
    hints: [
      'The pod is Pending. Check events:<code>kubectl describe pod -n day18 affinity-app</code>What does it say about scheduling?',
      'The pod requires a node with label gpu=true. Check your nodes:<code>kubectl get nodes --show-labels</code>No nodes have this label.',
      'Either label a node:<code>kubectl label node <node-name> gpu=true</code>Or change the pod\'s affinity to match an existing label.',
    ],
    docs: [
      { title: 'Node Affinity', url: 'https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity' },
      { title: 'Assigning Pods to Nodes', url: 'https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/' },
    ],
  },
  {
    day: 19,
    enabled: true,
    title: 'Ingress + TLS',
    description: 'An Ingress with TLS is configured but the Certificate won\'t issue because the issuer reference is wrong. Fix the cert-manager configuration.',
    flagHash: '89984116e4c89d895e53166e1f532b8fb2c9c50cc22beb94b04e96e7a6901e5b',
    setup: [
      'Install cert-manager:',
      '<code>helm repo add jetstack https://charts.jetstack.io\nhelm install cert-manager jetstack/cert-manager -n cert-manager --create-namespace --set crds.enabled=true</code>',
      'Install the chart:',
      '<code>helm install day19 oci://ghcr.io/adventofkube/charts/day19 --version 0.1.0</code>',
      'Check the certificate:',
      '<code>kubectl get certificate -n day19</code>',
    ],
    hints: [
      'The Certificate is not Ready. Check its status:<code>kubectl describe certificate flag-cert -n day19</code>What does the error say?',
      'The Certificate references issuer "letsencrypt-prod" but the actual ClusterIssuer is "selfsigned-issuer".<code>kubectl get clusterissuer</code>',
      'Fix the Certificate and Ingress annotation to reference "selfsigned-issuer" instead of "letsencrypt-prod".',
    ],
    docs: [
      { title: 'cert-manager', url: 'https://cert-manager.io/docs/' },
      { title: 'Ingress TLS', url: 'https://kubernetes.io/docs/concepts/services-networking/ingress/#tls' },
    ],
  },
  {
    day: 20,
    enabled: true,
    title: 'HPA Not Scaling',
    description: 'The HPA can\'t scale the deployment because it can\'t calculate CPU utilization. Fix the resource configuration so the app scales and reveals the flag.',
    flagHash: '81a405a0379643675e9f9cd857f0d26b68f2ae69620635afefcce5d854ba519e',
    setup: [
      'Install metrics-server (if not already):',
      '<code>kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\nkubectl patch -n kube-system deployment metrics-server --type=json -p \'[{"op":"add","path":"/spec/template/spec/containers/0/args/-","value":"--kubelet-insecure-tls"}]\'</code>',
      'Install the chart:',
      '<code>helm install day20 oci://ghcr.io/adventofkube/charts/day20 --version 0.1.0</code>',
      'Check the HPA:',
      '<code>kubectl get hpa -n day20</code>',
    ],
    hints: [
      'The HPA shows <unknown>/50% for CPU. It can\'t calculate utilization. Why?<code>kubectl describe hpa -n day20</code>',
      'The deployment has no CPU resource requests. HPA needs resource requests to calculate percentage utilization.',
      'Add resource requests to the deployment:<code>kubectl patch deployment scale-app -n day20 -p \'{"spec":{"template":{"spec":{"containers":[{"name":"scale-app","resources":{"requests":{"cpu":"50m"}}}]}}}}\'</code>The app generates CPU load, so it will scale once requests are set. The flag appears when >=2 replicas are ready.',
    ],
    docs: [
      { title: 'Horizontal Pod Autoscaling', url: 'https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/' },
      { title: 'HPA Walkthrough', url: 'https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/' },
    ],
  },
  {
    day: 21,
    enabled: true,
    title: 'GitOps Drift',
    description: 'An ArgoCD Application can\'t find its manifests because the source path is wrong. Fix the Application to sync successfully.',
    flagHash: 'ca681497e642ed59dba8f65f00378ed549b27d5ef19504fc19ace3869afb4414',
    setup: [
      'Install ArgoCD:',
      '<code>kubectl create namespace argocd\nkubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml</code>',
      'Install the chart:',
      '<code>helm install day21 oci://ghcr.io/adventofkube/charts/day21 --version 0.1.0</code>',
      'Check the Application:',
      '<code>kubectl get application -n argocd</code>',
    ],
    hints: [
      'The Application shows a sync error. Check its status:<code>kubectl describe application gitops-app -n argocd</code>What path is it looking at?',
      'The source path is "deploy/manifests" which doesn\'t exist in the repo. Check the hint ConfigMap for clues.',
      'Fix the Application spec.source.path to point to the correct location in the repo.',
    ],
    docs: [
      { title: 'ArgoCD Getting Started', url: 'https://argo-cd.readthedocs.io/en/stable/getting_started/' },
      { title: 'ArgoCD Application', url: 'https://argo-cd.readthedocs.io/en/stable/operator-manual/declarative-setup/' },
    ],
  },
  {
    day: 22,
    enabled: true,
    title: 'Sync Hooks',
    description: 'An ArgoCD PreSync hook Job is failing, blocking the entire sync. The main app never deploys. Fix the hook to unblock.',
    flagHash: '024ffb2f91c842fb20b21b5738ed9f431c5e6810c2268ed0a59a4b47891ac76e',
    setup: [
      'Ensure ArgoCD is installed (see Day 21).',
      'Install the chart:',
      '<code>helm install day22 oci://ghcr.io/adventofkube/charts/day22 --version 0.1.0</code>',
      'Check the Job status:',
      '<code>kubectl get jobs -n day22</code>',
    ],
    hints: [
      'The PreSync Job is failing. Check the pod status:<code>kubectl describe job db-migrate -n day22</code>What\'s the error?',
      'The Job uses image busybox:99.99 which doesn\'t exist. It\'s stuck in ImagePullBackOff.',
      'Fix the Job image to a valid version (e.g., busybox:1.36). Delete the failed Job, then trigger a sync to retry.',
    ],
    docs: [
      { title: 'ArgoCD Resource Hooks', url: 'https://argo-cd.readthedocs.io/en/stable/user-guide/resource_hooks/' },
      { title: 'Jobs', url: 'https://kubernetes.io/docs/concepts/workloads/controllers/job/' },
    ],
  },
  {
    day: 23,
    enabled: true,
    title: 'Policy Blocked',
    description: 'A Kyverno policy is blocking pod creation because a required label is missing. Fix the Deployment to comply.',
    flagHash: '0096ce6bb2184ec0be4da2fb24f8860fb400d25692332762b5d7856403e4eec7',
    setup: [
      'Install Kyverno:',
      '<code>helm repo add kyverno https://kyverno.github.io/kyverno\nhelm install kyverno kyverno/kyverno -n kyverno --create-namespace</code>',
      'Install the chart:',
      '<code>helm install day23 oci://ghcr.io/adventofkube/charts/day23 --version 0.1.0</code>',
      'Check pod creation:',
      '<code>kubectl get pods -n day23\nkubectl get events -n day23 --sort-by=.lastTimestamp</code>',
    ],
    hints: [
      'Pods aren\'t being created. Check the events for policy violations:<code>kubectl get events -n day23 | grep -i kyverno</code>',
      'A ClusterPolicy requires all pods in this namespace to have a "team" label. The Deployment only has "app".',
      'Add the team label to the pod template:<code>kubectl patch deployment policy-app -n day23 -p \'{"spec":{"template":{"metadata":{"labels":{"team":"platform"}}}}}\'</code>',
    ],
    docs: [
      { title: 'Kyverno Policies', url: 'https://kyverno.io/docs/writing-policies/' },
      { title: 'Kyverno Validate Rules', url: 'https://kyverno.io/docs/writing-policies/validate/' },
    ],
  },
  {
    day: 24,
    enabled: true,
    title: 'Webhook Woes',
    description: 'A ValidatingWebhook is rejecting your app\'s pods. Understand what the webhook requires and fix the Deployment to pass validation.',
    flagHash: 'b564e0f99af733b4d9f3aa0605521fe2cfce6463a62eb99339272b25cebaf194',
    setup: [
      'Install the chart:',
      '<code>helm install day24 oci://ghcr.io/adventofkube/charts/day24 --version 0.1.0</code>',
      'Wait for the webhook to be ready, then check pods:',
      '<code>kubectl get pods -n day24\nkubectl get events -n day24 --sort-by=.lastTimestamp</code>',
    ],
    hints: [
      'The app deployment\'s pods are being rejected. Check the events for admission errors:<code>kubectl get events -n day24 | grep -i rejected</code>What does the webhook say?',
      'The webhook rejects pods without annotation "approved: true". Check the webhook configuration:<code>kubectl get validatingwebhookconfiguration day24-approval-check -o yaml</code>',
      'Add the annotation to the Deployment pod template:<code>kubectl patch deployment webhook-app -n day24 -p \'{"spec":{"template":{"metadata":{"annotations":{"approved":"true"}}}}}\'</code>',
    ],
    docs: [
      { title: 'Dynamic Admission Control', url: 'https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/' },
      { title: 'ValidatingWebhookConfiguration', url: 'https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#webhook-configuration' },
    ],
  },
  {
    day: 25,
    enabled: true,
    title: 'Grand Finale',
    description: 'Three namespaces, seven bugs, one flag. RBAC is wrong, NetworkPolicies block everything, pods are misconfigured. Fix all three services and combine their fragments.',
    flagHash: '7e73dc6fbc809d72f6146c97c110d425a0fca511dac070ece2946d5df868651f',
    setup: [
      'Install the chart:',
      '<code>helm install day25 oci://ghcr.io/adventofkube/charts/day25 --version 0.1.0</code>',
      'Check all three namespaces:',
      '<code>kubectl get pods -n finale-a\nkubectl get pods -n finale-b\nkubectl get pods -n finale-c</code>',
      'Each service returns a flag fragment. Combine all three for the full flag.',
    ],
    hints: [
      'Start with each namespace separately. Use <code>kubectl describe</code> and <code>kubectl logs</code> to identify the issues in each.',
      'finale-a has an RBAC issue (check the Role verbs). finale-b has a NetworkPolicy + Service port mismatch. finale-c has a missing ConfigMap + wrong liveness path + OOMKilled.',
      'Fix each namespace\'s issues, curl each service, and concatenate the three fragments to get the complete flag.',
    ],
    docs: [
      { title: 'Debugging Pods', url: 'https://kubernetes.io/docs/tasks/debug/debug-application/debug-pods/' },
      { title: 'Debugging Services', url: 'https://kubernetes.io/docs/tasks/debug/debug-application/debug-service/' },
      { title: 'RBAC', url: 'https://kubernetes.io/docs/reference/access-authn-authz/rbac/' },
      { title: 'Network Policies', url: 'https://kubernetes.io/docs/concepts/services-networking/network-policies/' },
    ],
  },
];
