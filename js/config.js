// Set the start date for day unlocking.
// Day 1 unlocks on START_DATE, Day 2 on START_DATE + 1 day, etc.
export const START_DATE = new Date('2026-01-05T00:00:00');

// Per-day configuration. flagHash is the SHA-256 hex digest of the correct flag.
export const DAYS = [
  {
    day: 0,
    enabled: true,
    title: 'Setup',
    description: 'Set up your local environment, deploy a test chart, and retrieve the flag from the pod logs.',
    flagHash: '225840fef30125024efb75a3dee9d054dc23deb7f5cee632a75252337f795e18',
    chartUrl: 'https://example.com/charts/day00.tgz',
    osSetup: {
      windows: [
        { title: '1. Install Docker Desktop', content: 'Download and install <a href="https://docs.docker.com/desktop/install/windows-install/" target="_blank" rel="noopener">Docker Desktop</a>. Enable WSL2 when prompted.' },
        { title: '2. Install kubectl', content: 'Download <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/" target="_blank" rel="noopener">kubectl.exe</a> and add it to your PATH, or run in PowerShell:<code>winget install Kubernetes.kubectl</code>' },
        { title: '3. Install kind', content: 'Download from <a href="https://kind.sigs.k8s.io/docs/user/quick-start/#installing-from-release-binaries" target="_blank" rel="noopener">kind releases</a> and add to PATH, or run:<code>winget install Kubernetes.kind</code>' },
        { title: '4. Install Helm', content: 'Download from <a href="https://helm.sh/docs/intro/install/#from-the-binary-releases" target="_blank" rel="noopener">Helm releases</a> and add to PATH, or run:<code>winget install Helm.Helm</code>' },
      ],
      mac: [
        { title: '1. Install Docker Desktop', content: '<code>brew install --cask docker</code> or download from <a href="https://docs.docker.com/desktop/install/mac-install/" target="_blank" rel="noopener">Docker Desktop</a>' },
        { title: '2. Install kubectl', content: '<code>brew install kubectl</code>' },
        { title: '3. Install kind', content: '<code>brew install kind</code>' },
        { title: '4. Install Helm', content: '<code>brew install helm</code>' },
      ],
      linux: [
        { title: '1. Install Docker Engine', content: 'Follow the <a href="https://docs.docker.com/engine/install/" target="_blank" rel="noopener">Docker Engine install guide</a> for your distro, then run <code>sudo systemctl start docker</code>' },
        { title: '2. Install kubectl', content: 'Follow the <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/" target="_blank" rel="noopener">kubectl install guide</a>' },
        { title: '3. Install kind', content: 'Follow the <a href="https://kind.sigs.k8s.io/docs/user/quick-start/#installation" target="_blank" rel="noopener">kind install guide</a>' },
        { title: '4. Install Helm', content: 'Follow the <a href="https://helm.sh/docs/intro/install/" target="_blank" rel="noopener">Helm install guide</a>' },
      ],
    },
    setup: [
      '<strong>5. Verify installations</strong>',
      '<code>docker --version\nkubectl version --client\nkind --version\nhelm version</code>',
      '<strong>6. Create a kind cluster</strong>',
      '<code>kind create cluster --name adventofkube</code>',
      '<strong>7. Install the Day 0 chart</strong>',
      '<code>helm install day00 oci://ghcr.io/adventofkube/charts/day00 --version 0.2.0</code>',
      '<strong>8. Get the flag from the pod logs</strong>',
      '<code>kubectl logs -n day00 setup-check</code>',
    ],
    hints: [
      'Make sure Docker is running before creating the kind cluster.',
      'If <code>kind create cluster</code> fails, try <code>docker ps</code> to check that Docker is responsive.',
      'On Linux, you may need to run Docker commands with <code>sudo</code> or add your user to the <code>docker</code> group.',
      'If the pod isn\'t showing logs yet, run <code>kubectl get pods -n day00</code> and wait for it to complete.',
    ],
    docs: [
      { title: 'Install kubectl', url: 'https://kubernetes.io/docs/tasks/tools/' },
      { title: 'kind Quick Start', url: 'https://kind.sigs.k8s.io/docs/user/quick-start/' },
      { title: 'Install Helm', url: 'https://helm.sh/docs/intro/install/' },
      { title: 'kubectl Cheat Sheet', url: 'https://kubernetes.io/docs/reference/kubectl/cheatsheet/' },
    ],
  },
  {
    day: 1,
    enabled: true,
    title: 'Pods',
    description: 'A pod won\'t start. Investigate the issue, fix it, and retrieve the flag from the logs.',
    flagHash: '32c1012bb9a9c6b1dcd21bf9e6e78a05aaa201eda110e958908f5cdecd9b9317',
    chartUrl: 'https://example.com/charts/day01.tgz',
    setup: [
      'Install the chart:',
      '<code>helm install day01 oci://ghcr.io/adventofkube/charts/day01 --version 0.2.0</code>',
      'Start investigating:',
      '<code>kubectl get pods -n day01</code>',
      'Once the pod is running, get the flag:',
      '<code>kubectl logs -n day01 day01-pod</code>',
    ],
    hints: [
      'Check the pod status and events:<code>kubectl get pods -n day01\nkubectl describe pod day01-pod -n day01</code>Look at the Events section at the bottom.',
      'The Events mention a failed image pull. Look closely at the image name — does it look right?',
      'The image name is <code>daay01</code> (with two a\'s). It should be <code>day01</code>. Delete the pod and recreate it with the correct image:<code>kubectl get pod day01-pod -n day01 -o yaml > fix.yaml\n# Edit fix.yaml: change daay01 to day01\nkubectl delete pod day01-pod -n day01\nkubectl apply -f fix.yaml</code>',
    ],
    docs: [
      { title: 'kubectl get', url: 'https://kubernetes.io/docs/reference/kubectl/generated/kubectl_get/' },
      { title: 'kubectl describe', url: 'https://kubernetes.io/docs/reference/kubectl/generated/kubectl_describe/' },
      { title: 'kubectl logs', url: 'https://kubernetes.io/docs/reference/kubectl/generated/kubectl_logs/' },
      { title: 'Debugging Pods', url: 'https://kubernetes.io/docs/tasks/debug/debug-application/debug-pods/' },
    ],
  },
  {
    day: 2,
    enabled: true,
    title: 'ConfigMaps',
    description: 'A pod is stuck and won\'t start. Something is wrong with its configuration. Find and fix the issue.',
    flagHash: '0d91a360ae0f4a3e11c28f6db96ff10dc29a7e0e5f8bafaa6a14472b6961a826',
    chartUrl: 'https://example.com/charts/day02.tgz',
    setup: [
      'Install the chart:',
      '<code>helm install day02 oci://ghcr.io/adventofkube/charts/day02 --version 0.3.0</code>',
      'Check the pod status:',
      '<code>kubectl get pods -n day02</code>',
      'The pod is stuck. Investigate why:',
      '<code>kubectl describe pod day02-pod -n day02</code>',
      'Once the pod is running, get the flag:',
      '<code>kubectl logs -n day02 day02-pod</code>',
    ],
    hints: [
      'The pod status shows CreateContainerConfigError. Look at the Events section in <code>kubectl describe</code> — what does the error message say?',
      'The error mentions a ConfigMap that doesn\'t exist. The pod expects <code>envFrom</code> to load from a ConfigMap. Check which one:<code>kubectl get pod day02-pod -n day02 -o yaml | grep configMapRef -A 1</code>',
      'The pod references <code>day02-settings</code>, but the actual ConfigMap is named <code>day02-config</code>:<code>kubectl get configmap -n day02</code>Export the pod, fix the reference, delete and recreate:<code>kubectl get pod day02-pod -n day02 -o yaml > fix.yaml\n# Edit fix.yaml: change day02-settings to day02-config\nkubectl delete pod day02-pod -n day02\nkubectl apply -f fix.yaml</code>',
    ],
    docs: [
      { title: 'kubectl logs', url: 'https://kubernetes.io/docs/reference/kubectl/generated/kubectl_logs/' },
      { title: 'Debugging Pods', url: 'https://kubernetes.io/docs/tasks/debug/debug-application/debug-pods/' },
      { title: 'ConfigMaps', url: 'https://kubernetes.io/docs/concepts/configuration/configmap/' },
      { title: 'Define Environment Variables Using a ConfigMap', url: 'https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/' },
    ],
  },
  {
    day: 3,
    enabled: true,
    title: 'Services',
    description: 'A Service can\'t reach its backend. The pod is running, but something is wrong. Debug the connectivity.',
    flagHash: 'a0440a763fbd9cc4de34e9245f9ec0840449afd3cfdd9493b988fd4c636c3d8d',
    chartUrl: 'https://example.com/charts/day03.tgz',
    setup: [
      'Install the chart:',
      '<code>helm install day03 oci://ghcr.io/adventofkube/charts/day03 --version 0.3.0</code>',
      'Check the pod is running:',
      '<code>kubectl get pods -n day03</code>',
      'Try to reach the service (this will fail initially):',
      '<code>kubectl run curl --rm -it --image=curlimages/curl --restart=Never -n day03 -- curl -m 5 http://flag-server</code>',
      'The <code>-m 5</code> sets a 5-second timeout. Once the service is fixed, it will return the flag.',
    ],
    hints: [
      'The pod is running, but the Service can\'t reach it. Check the Service endpoints:<code>kubectl get endpoints -n day03</code>If it shows no endpoints, the Service isn\'t finding any pods.',
      'Services find pods using label selectors. Does this Service have a selector?<code>kubectl get svc flag-server -n day03 -o yaml</code>Check what labels the Pod has:<code>kubectl get pods -n day03 --show-labels</code>',
      'The Service has no selector! You can fix this by editing the Service to add one:<code>kubectl edit svc flag-server -n day03</code>Or delete it and use <code>kubectl expose</code>:<code>kubectl delete svc flag-server -n day03</code><code>kubectl expose deployment flag-server -n day03 --port=80 --target-port=8080</code>',
    ],
    docs: [
      { title: 'Services', url: 'https://kubernetes.io/docs/concepts/services-networking/service/' },
      { title: 'Labels and Selectors', url: 'https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/' },
      { title: 'kubectl expose', url: 'https://kubernetes.io/docs/reference/kubectl/generated/kubectl_expose/' },
      { title: 'Debugging Services', url: 'https://kubernetes.io/docs/tasks/debug/debug-application/debug-service/' },
    ],
  },
  {
    day: 4,
    enabled: true,
    title: 'Secrets',
    description: 'A pod won\'t start. It needs a Secret, but something isn\'t configured correctly. Figure out what\'s wrong.',
    flagHash: '54ab235edd9d3c53a882665d54308b0eb90d078f027797a9cee8424fea98143d',
    chartUrl: 'https://example.com/charts/day04.tgz',
    setup: [
      'Install the chart:',
      '<code>helm install day04 oci://ghcr.io/adventofkube/charts/day04 --version 0.1.0</code>',
      'Check the pod status:',
      '<code>kubectl get pods -n day04</code>',
      'The pod is stuck. Investigate why:',
      '<code>kubectl describe pod secret-app -n day04</code>',
      'Once the pod runs successfully, get the flag:',
      '<code>kubectl logs -n day04 secret-app</code>',
    ],
    hints: [
      'The pod status shows CreateContainerConfigError. The Events section in <code>kubectl describe</code> shows an error about a missing Secret key.',
      'The error says key <code>API_KEY</code> not found in Secret. Check what keys the Secret actually has:<code>kubectl get secret app-credentials -n day04 -o jsonpath=\'{.data}\'</code>You\'ll see the key names (the values are base64 encoded, but key names are visible).',
      'The Secret has key <code>api-key</code> but the Pod expects <code>API_KEY</code>. Fix the Secret:<code>kubectl get secret app-credentials -n day04 -o yaml > fix.yaml\n# Edit fix.yaml: rename the key from api-key to API_KEY\nkubectl apply -f fix.yaml\nkubectl delete pod secret-app -n day04</code>The pod will be recreated automatically.',
    ],
    docs: [
      { title: 'Secrets', url: 'https://kubernetes.io/docs/concepts/configuration/secret/' },
      { title: 'Using Secrets as Environment Variables', url: 'https://kubernetes.io/docs/concepts/configuration/secret/#using-secrets-as-environment-variables' },
      { title: 'kubectl describe', url: 'https://kubernetes.io/docs/reference/kubectl/generated/kubectl_describe/' },
    ],
  },
  {
    day: 5,
    enabled: true,
    title: 'Resource Quotas',
    description: 'A Deployment exists but no pods are being created. Something is preventing the pods from being scheduled.',
    flagHash: '5629ebc1e04418970d237dd69df2ad81797cdc35087357879d3ed8a433bc0e4e',
    chartUrl: 'https://example.com/charts/day05.tgz',
    setup: [
      'Install the chart:',
      '<code>helm install day05 oci://ghcr.io/adventofkube/charts/day05 --version 0.1.0</code>',
      'Check the deployment:',
      '<code>kubectl get deployments -n day05</code>',
      'Notice the deployment has 0 pods ready. Investigate why:',
      '<code>kubectl describe deployment quota-app -n day05</code>',
      'Once a pod is running, get the flag:',
      '<code>kubectl logs -n day05 -l app=quota-app</code>',
    ],
    hints: [
      'The Deployment shows 0/1 replicas ready, but there are no pods. Check the ReplicaSet events:<code>kubectl get rs -n day05\nkubectl describe rs -n day05</code>Look at the Events section for errors.',
      'The error mentions "exceeded quota". Check the namespace\'s ResourceQuota:<code>kubectl get resourcequota -n day05\nkubectl describe resourcequota day05-quota -n day05</code>Compare the quota limits to what the Deployment requests.',
      'The quota allows 200m CPU / 128Mi memory for requests, but the Deployment requests 500m / 256Mi. Edit the Deployment to reduce resources:<code>kubectl edit deployment quota-app -n day05</code>Set requests to cpu: 100m, memory: 64Mi (and matching limits within quota).',
    ],
    docs: [
      { title: 'Resource Quotas', url: 'https://kubernetes.io/docs/concepts/policy/resource-quotas/' },
      { title: 'Managing Resources for Containers', url: 'https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/' },
      { title: 'kubectl describe', url: 'https://kubernetes.io/docs/reference/kubectl/generated/kubectl_describe/' },
    ],
  },
  {
    day: 6,
    enabled: true,
    title: 'Deployments',
    description: 'A Deployment is failing to roll out. There are 3 things wrong. Diagnose each issue and fix them to get the flag.',
    flagHash: '78b10c3f6c1040d28782d9761ab28cf0cb3f240d8c4eb03e8e8ca8b65109a23b',
    chartUrl: 'https://example.com/charts/day06.tgz',
    setup: [
      'Install the chart:',
      '<code>helm install day06 oci://ghcr.io/adventofkube/charts/day06 --version 0.2.0</code>',
      'Start investigating:',
      '<code>kubectl get pods -n day06</code>',
      'Once all issues are fixed, curl the service for the flag:',
      '<code>kubectl run curl --rm -it --image=curlimages/curl --restart=Never -n day06 -- curl -m 5 http://day06</code>',
    ],
    hints: [
      'Check the pod status and events:<code>kubectl describe pod -n day06 -l app=day06</code>Look at the Events section — what does it say about the image?',
      'The image tag doesn\'t exist in the registry. Edit the Deployment to use a valid tag:<code>kubectl edit deployment day06 -n day06</code>',
      'After fixing the image, the pod crashes. Check the logs:<code>kubectl logs -n day06 -l app=day06</code>The error message tells you exactly what\'s missing.',
      'The app needs an environment variable. There\'s a ConfigMap in the namespace:<code>kubectl get configmap -n day06 -o yaml</code>Add an <code>envFrom</code> block to the Deployment referencing the ConfigMap.',
      'The pod is running but the Service can\'t reach it. Check the Service ports:<code>kubectl get svc day06 -n day06 -o yaml</code>The app listens on <code>8080</code>. Fix the <code>targetPort</code>.',
    ],
    docs: [
      { title: 'Deployments', url: 'https://kubernetes.io/docs/concepts/workloads/controllers/deployment/' },
      { title: 'ConfigMaps as Env Vars', url: 'https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/#configure-all-key-value-pairs-in-a-configmap-as-container-environment-variables' },
      { title: 'Service Ports', url: 'https://kubernetes.io/docs/concepts/services-networking/service/#defining-a-service' },
      { title: 'kubectl describe', url: 'https://kubernetes.io/docs/reference/kubectl/generated/kubectl_describe/' },
    ],
  },
  {
    day: 7,
    enabled: true,
    title: 'Service Wiring',
    description: 'A Service can\'t route to its pod. The deployment is running but the Service port configuration is wrong. Fix the wiring.',
    flagHash: '85a040e670c7fa404628d5ea8fe7d5fbf582f9dcd501eebc4e3025fc41c28f0b',
    setup: [
      'Install the chart:',
      '<code>helm install day07 oci://ghcr.io/adventofkube/charts/day07 --version 0.1.0</code>',
      'Test the service:',
      '<code>kubectl run curl --rm -it --image=curlimages/curl --restart=Never -n day07 -- curl -m 5 http://flag-server</code>',
    ],
    hints: [
      'The Service exists and the pod is running. Check if the Service has endpoints:<code>kubectl get endpoints -n day07</code>',
      'The endpoints exist but the Service can\'t connect. Compare the Service targetPort with the container port name:<code>kubectl get svc flag-server -n day07 -o yaml\nkubectl get pods -n day07 -o yaml | grep -A2 ports</code>',
      'The Service targetPort is "https" but the container port is named "web". Change the targetPort to "web" or to 8080.',
    ],
    docs: [
      { title: 'Services', url: 'https://kubernetes.io/docs/concepts/services-networking/service/' },
      { title: 'Debugging Services', url: 'https://kubernetes.io/docs/tasks/debug/debug-application/debug-service/' },
    ],
  },
  {
    day: 8,
    enabled: true,
    title: 'Probe Pitfall',
    description: 'A pod keeps restarting and is never ready. The health checks are misconfigured in multiple ways. Fix them all.',
    flagHash: '57d08d666f9ccf05e1ec23ef83719b85c27e28c72500bd57ca3eaa9f487f1224',
    setup: [
      'Install the chart:',
      '<code>helm install day08 oci://ghcr.io/adventofkube/charts/day08 --version 0.1.0</code>',
      'Watch the pod status:',
      '<code>kubectl get pods -n day08 -w</code>',
      'Once the pod is stable, curl the service:',
      '<code>kubectl run curl --rm -it --image=curlimages/curl --restart=Never -n day08 -- curl -m 5 http://probe-app</code>',
    ],
    hints: [
      'The pod keeps restarting. Check events:<code>kubectl describe pod -n day08 -l app=probe-app</code>The liveness probe is failing — what path does it check?',
      'The liveness probe checks /healthz but the app only serves /health. Fix the liveness path. But there are more issues...',
      'The readiness probe fires immediately (0s delay) but the app takes 3 seconds to start. Also, periodSeconds=1 and failureThreshold=1 are too aggressive. Fix all three: liveness path, readiness initialDelay, and readiness threshold.',
    ],
    docs: [
      { title: 'Configure Liveness & Readiness Probes', url: 'https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/' },
      { title: 'Pod Lifecycle', url: 'https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/' },
    ],
  },
  {
    day: 9,
    enabled: true,
    title: 'RBAC Lockdown',
    description: 'A pod needs to list Secrets but its ServiceAccount doesn\'t have the right permissions. Fix the RBAC configuration.',
    flagHash: '42107fc241bc8ec3883f137bc3840cae4b1280bdecd677be98fd32d7c2e6c008',
    setup: [
      'Install the chart:',
      '<code>helm install day09 oci://ghcr.io/adventofkube/charts/day09 --version 0.1.0</code>',
      'Check the pod logs:',
      '<code>kubectl logs -n day09 rbac-app</code>',
    ],
    hints: [
      'The pod logs show "Cannot list secrets". Check the Role:<code>kubectl get role -n day09 -o yaml</code>What verbs are allowed?',
      'The Role only has "get" but the app calls List(). Add "list" to the Role verbs.',
      'Edit the Role:<code>kubectl edit role secret-reader -n day09</code>Add "list" to the verbs array, then delete the pod to restart it.',
    ],
    docs: [
      { title: 'RBAC Authorization', url: 'https://kubernetes.io/docs/reference/access-authn-authz/rbac/' },
      { title: 'Using RBAC', url: 'https://kubernetes.io/docs/reference/access-authn-authz/rbac/#role-and-clusterrole' },
    ],
  },
  {
    day: 10,
    enabled: true,
    title: 'NetworkPolicy Firewall',
    description: 'A pod has the flag but a NetworkPolicy is blocking all traffic. Fix the policy to allow access.',
    flagHash: '066bc9777974a3af3d9deb1beaf96d43e6b4d184253cb58eaac83c333a531a44',
    setup: [
      'Install the chart:',
      '<code>helm install day10 oci://ghcr.io/adventofkube/charts/day10 --version 0.1.0</code>',
      'Try to reach the service:',
      '<code>kubectl run curl --rm -it --image=curlimages/curl --restart=Never -n day10 -- curl -m 5 http://flag-server</code>',
    ],
    hints: [
      'The curl times out. Check if there\'s a NetworkPolicy:<code>kubectl get networkpolicy -n day10 -o yaml</code>',
      'There\'s a default-deny policy blocking all ingress. You need to add an ingress allow rule.',
      'Either delete the deny policy or create a new one that allows ingress on port 8080:<code>kubectl delete networkpolicy default-deny -n day10</code>',
    ],
    docs: [
      { title: 'Network Policies', url: 'https://kubernetes.io/docs/concepts/services-networking/network-policies/' },
      { title: 'Declare Network Policy', url: 'https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/' },
    ],
  },
  {
    day: 11,
    enabled: true,
    title: 'PV/PVC Binding',
    description: 'A PersistentVolumeClaim won\'t bind to its PersistentVolume. The pod is stuck waiting for storage. Fix the binding.',
    flagHash: '49e0035adeb7fdd1d36c4e8183bc38897da95415e5f19418af277607e4d73e24',
    setup: [
      'Install the chart:',
      '<code>helm install day11 oci://ghcr.io/adventofkube/charts/day11 --version 0.1.0</code>',
      'Check the PVC status:',
      '<code>kubectl get pvc -n day11</code>',
      'Once the pod runs, check its logs:',
      '<code>kubectl logs -n day11 storage-app</code>',
    ],
    hints: [
      'The PVC is Pending. Compare the PVC and PV:<code>kubectl get pv\nkubectl get pvc -n day11 -o yaml</code>Do the storageClassNames match?',
      'The PVC requests storageClassName "standard" but the PV uses "manual". Also check the requested size.',
      'Fix both: change PVC storageClassName to "manual" and reduce request from 2Gi to 1Gi (or less) to match the PV capacity.',
    ],
    docs: [
      { title: 'Persistent Volumes', url: 'https://kubernetes.io/docs/concepts/storage/persistent-volumes/' },
      { title: 'Configure a Pod to Use a PV', url: 'https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/' },
    ],
  },
  {
    day: 12,
    enabled: true,
    title: 'Rolling Update Stuck',
    description: 'A Deployment rollout is deadlocked. Fix the strategy, then update the image to v2 to get the flag.',
    flagHash: '1463a749773ed54b90300bdeaa97241e537dc84d8f9b3898decaf966122665b2',
    setup: [
      'Install the chart:',
      '<code>helm install day12 oci://ghcr.io/adventofkube/charts/day12 --version 0.1.0</code>',
      'Check the current version:',
      '<code>kubectl run curl --rm -it --image=curlimages/curl --restart=Never -n day12 -- curl -m 5 http://rollout-app</code>',
      'You need to update to v2 to get the flag.',
    ],
    hints: [
      'Try updating the image:<code>kubectl set image deployment/rollout-app rollout-app=ghcr.io/adventofkube/day12:v2 -n day12</code>Then watch the rollout:<code>kubectl rollout status deployment/rollout-app -n day12</code>It gets stuck. Why?',
      'Check the deployment strategy:<code>kubectl get deployment rollout-app -n day12 -o yaml | grep -A3 strategy</code>Both maxSurge and maxUnavailable are 0 — deadlock.',
      'Fix the strategy first:<code>kubectl patch deployment rollout-app -n day12 -p \'{"spec":{"strategy":{"rollingUpdate":{"maxSurge":1}}}}\'</code>Then the rollout to v2 will proceed.',
    ],
    docs: [
      { title: 'Deployments - Rolling Update', url: 'https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#rolling-update-deployment' },
      { title: 'kubectl rollout', url: 'https://kubernetes.io/docs/reference/kubectl/generated/kubectl_rollout/' },
    ],
  },
  {
    day: 13,
    enabled: true,
    title: 'Metrics Missing',
    description: 'Prometheus isn\'t scraping your app\'s metrics. The ServiceMonitor configuration has a mismatch. Fix it to see the flag in the metrics.',
    flagHash: 'df2ea19e39166c0ea3fa48ddec702215e2a077245c65301c5904a36cb4daa580',
    setup: [
      'Install Prometheus (if not already):',
      '<code>helm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm install prometheus prometheus-community/kube-prometheus-stack -n monitoring --create-namespace</code>',
      'Install the chart:',
      '<code>helm install day13 oci://ghcr.io/adventofkube/charts/day13 --version 0.1.0</code>',
      'Check the app\'s /metrics endpoint directly:',
      '<code>kubectl run curl --rm -it --image=curlimages/curl --restart=Never -n day13 -- curl -m 5 http://metrics-app/metrics</code>',
    ],
    hints: [
      'The metrics endpoint works directly but Prometheus isn\'t scraping it. Check the ServiceMonitor:<code>kubectl get servicemonitor -n day13 -o yaml</code>Compare its matchLabels with the Service labels.',
      'The ServiceMonitor selects "metrics-server" but the Service label is "metrics-app". Fix the ServiceMonitor selector.',
      'The flag is in the HELP text of the flag_value metric. Once Prometheus scrapes it, query: <code>flag_value</code> and look at the HELP description.',
    ],
    docs: [
      { title: 'ServiceMonitor', url: 'https://prometheus-operator.dev/docs/user-guides/getting-started/#deploying-a-sample-application' },
      { title: 'Prometheus Operator', url: 'https://prometheus-operator.dev/docs/prologue/introduction/' },
    ],
  },
  {
    day: 14,
    enabled: true,
    title: 'Dashboard Down',
    description: 'A Grafana dashboard shows "No Data" because the datasource URL is wrong. Fix the configuration to see the flag.',
    flagHash: 'a96e749b5fa79392ee39c9648dd5dcf054aee9a24ee6437e010bf806f8d32019',
    setup: [
      'Ensure Prometheus is installed (see Day 13).',
      'Install the chart:',
      '<code>helm install day14 oci://ghcr.io/adventofkube/charts/day14 --version 0.1.0</code>',
      'Port-forward Grafana:',
      '<code>kubectl port-forward svc/prometheus-grafana -n monitoring 3000:80</code>',
    ],
    hints: [
      'Open Grafana at localhost:3000. Find the "Advent of Kube - Day 14" dashboard. It shows "No Data".',
      'Check the datasource ConfigMap:<code>kubectl get configmap grafana-datasource -n day14 -o yaml</code>The Prometheus URL is wrong.',
      'Fix the datasource URL to match the actual Prometheus service name in your cluster (e.g., prometheus-kube-prometheus-prometheus.monitoring.svc:9090). The flag is in the dashboard panel title.',
    ],
    docs: [
      { title: 'Grafana Datasources', url: 'https://grafana.com/docs/grafana/latest/datasources/' },
      { title: 'Grafana Dashboard Provisioning', url: 'https://grafana.com/docs/grafana/latest/administration/provisioning/#dashboards' },
    ],
  },
  {
    day: 15,
    enabled: true,
    title: 'Alert Routing',
    description: 'An AlertManager alert fires correctly but the notification never reaches the webhook. The routing rules are misconfigured.',
    flagHash: '0a99d9a2b405e9512ad171339b454415739bea73de610cb6bddf9b95890de1a2',
    setup: [
      'Ensure Prometheus + AlertManager are installed (see Day 13).',
      'Install the chart:',
      '<code>helm install day15 oci://ghcr.io/adventofkube/charts/day15 --version 0.1.0</code>',
      'Verify the alert fires:',
      '<code>kubectl port-forward svc/prometheus-kube-prometheus-alertmanager -n monitoring 9093:9093</code>',
    ],
    hints: [
      'The FlagReady alert fires with label team=infra. Check the AlertManager routing config:<code>kubectl get secret alertmanager-config -n day15 -o jsonpath=\'{.data.alertmanager\\.yaml}\' | base64 -d</code>',
      'The route matches team=backend but the alert has team=infra. The alert falls through to the null receiver.',
      'Fix the route match label from "backend" to "infra" so alerts route to the flag-webhook receiver.',
    ],
    docs: [
      { title: 'AlertManager Routing', url: 'https://prometheus.io/docs/alerting/latest/configuration/#route' },
      { title: 'PrometheusRule', url: 'https://prometheus-operator.dev/docs/user-guides/alerting/' },
    ],
  },
  {
    day: 16,
    enabled: true,
    title: 'Logs Lost',
    description: 'Promtail is supposed to collect logs but the scrape path glob is wrong. Fix it to find the flag in the logs.',
    flagHash: '339b5b9e257354a8cd87302b6905c91cf7c1974d5a95c518f1a957365049e959',
    setup: [
      'Install Loki + Promtail:',
      '<code>helm repo add grafana https://grafana.github.io/helm-charts\nhelm install loki grafana/loki-stack -n monitoring --set promtail.enabled=true</code>',
      'Install the chart:',
      '<code>helm install day16 oci://ghcr.io/adventofkube/charts/day16 --version 0.1.0</code>',
    ],
    hints: [
      'The app prints the flag to stdout on startup. Check Promtail config:<code>kubectl get configmap promtail-config -n day16 -o yaml</code>Look at the __path__ glob.',
      'The scrape path is /var/log/pods/*/*/*.log but it should be /var/log/pods/*/*/*/*.log (4 levels, not 3).',
      'The Kubernetes log path format is: /var/log/pods/<namespace>_<pod>_<uid>/<container>/<restart_count>.log — that\'s 4 wildcards needed.',
    ],
    docs: [
      { title: 'Promtail Configuration', url: 'https://grafana.com/docs/loki/latest/send-data/promtail/configuration/' },
      { title: 'Kubernetes Logging', url: 'https://kubernetes.io/docs/concepts/cluster-administration/logging/' },
    ],
  },
  {
    day: 17,
    enabled: true,
    title: 'Pipeline Mismatch',
    description: 'Promtail\'s log parsing pipeline expects a different format than the app produces. Fix the pipeline to extract labels and find the flag.',
    flagHash: '07461bed4d8f32c1e60d7a34f36f7e1650595440ba819d73248e2e7caed857c5',
    setup: [
      'Ensure Loki + Promtail are installed (see Day 16).',
      'Install the chart:',
      '<code>helm install day17 oci://ghcr.io/adventofkube/charts/day17 --version 0.1.0</code>',
      'Query logs in Grafana Explore with LogQL:',
      '<code>{job="pipeline-app"}</code>',
    ],
    hints: [
      'The Promtail pipeline uses a regex stage expecting "level=INFO msg=..." format. Check what the app actually logs:<code>kubectl logs -n day17 -l app=pipeline-app</code>',
      'The app logs JSON: {"level":"info","msg":"..."}. The regex doesn\'t match JSON format, so labels are never extracted.',
      'Replace the regex stage with a json stage in the Promtail config, or fix the regex to match JSON. The flag is in one of the log messages.',
    ],
    docs: [
      { title: 'Promtail Pipelines', url: 'https://grafana.com/docs/loki/latest/send-data/promtail/pipelines/' },
      { title: 'Pipeline Stages', url: 'https://grafana.com/docs/loki/latest/send-data/promtail/stages/' },
    ],
  },
  {
    day: 18,
    enabled: true,
    title: 'Node Affinity',
    description: 'A pod is stuck Pending because its node affinity requires a label that no node has. Fix the scheduling constraint.',
    flagHash: '452c45b3d82f478ca254e331bc1e59083a8a9a5cce38bb51086eba18886ae3d7',
    setup: [
      'Install the chart:',
      '<code>helm install day18 oci://ghcr.io/adventofkube/charts/day18 --version 0.1.0</code>',
      'Check the pod status:',
      '<code>kubectl get pods -n day18</code>',
    ],
    hints: [
      'The pod is Pending. Check events:<code>kubectl describe pod -n day18 affinity-app</code>What does it say about scheduling?',
      'The pod requires a node with label gpu=true. Check your nodes:<code>kubectl get nodes --show-labels</code>No nodes have this label.',
      'Either label a node:<code>kubectl label node <node-name> gpu=true</code>Or change the pod\'s affinity to match an existing label.',
    ],
    docs: [
      { title: 'Node Affinity', url: 'https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity' },
      { title: 'Assigning Pods to Nodes', url: 'https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/' },
    ],
  },
  {
    day: 19,
    enabled: true,
    title: 'Ingress + TLS',
    description: 'An Ingress with TLS is configured but the Certificate won\'t issue because the issuer reference is wrong. Fix the cert-manager configuration.',
    flagHash: '074dbad7c7bdaf664992daafd340d3937ae9a3d9a081b38a85c530245cfa4ee0',
    setup: [
      'Install cert-manager:',
      '<code>helm repo add jetstack https://charts.jetstack.io\nhelm install cert-manager jetstack/cert-manager -n cert-manager --create-namespace --set crds.enabled=true</code>',
      'Install the chart:',
      '<code>helm install day19 oci://ghcr.io/adventofkube/charts/day19 --version 0.1.0</code>',
      'Check the certificate:',
      '<code>kubectl get certificate -n day19</code>',
    ],
    hints: [
      'The Certificate is not Ready. Check its status:<code>kubectl describe certificate flag-cert -n day19</code>What does the error say?',
      'The Certificate references issuer "letsencrypt-prod" but the actual ClusterIssuer is "selfsigned-issuer".<code>kubectl get clusterissuer</code>',
      'Fix the Certificate and Ingress annotation to reference "selfsigned-issuer" instead of "letsencrypt-prod".',
    ],
    docs: [
      { title: 'cert-manager', url: 'https://cert-manager.io/docs/' },
      { title: 'Ingress TLS', url: 'https://kubernetes.io/docs/concepts/services-networking/ingress/#tls' },
    ],
  },
  {
    day: 20,
    enabled: true,
    title: 'HPA Not Scaling',
    description: 'The HPA can\'t scale the deployment because it can\'t calculate CPU utilization. Fix the resource configuration so the app scales and reveals the flag.',
    flagHash: 'd343163ded2d0585daea10cb6226d56e06812537969b4c004e40d19483af5293',
    setup: [
      'Install metrics-server (if not already):',
      '<code>kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\nkubectl patch -n kube-system deployment metrics-server --type=json -p \'[{"op":"add","path":"/spec/template/spec/containers/0/args/-","value":"--kubelet-insecure-tls"}]\'</code>',
      'Install the chart:',
      '<code>helm install day20 oci://ghcr.io/adventofkube/charts/day20 --version 0.1.0</code>',
      'Check the HPA:',
      '<code>kubectl get hpa -n day20</code>',
    ],
    hints: [
      'The HPA shows <unknown>/50% for CPU. It can\'t calculate utilization. Why?<code>kubectl describe hpa -n day20</code>',
      'The deployment has no CPU resource requests. HPA needs resource requests to calculate percentage utilization.',
      'Add resource requests to the deployment:<code>kubectl patch deployment scale-app -n day20 -p \'{"spec":{"template":{"spec":{"containers":[{"name":"scale-app","resources":{"requests":{"cpu":"50m"}}}]}}}}\'</code>The app generates CPU load, so it will scale once requests are set. The flag appears when >=2 replicas are ready.',
    ],
    docs: [
      { title: 'Horizontal Pod Autoscaling', url: 'https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/' },
      { title: 'HPA Walkthrough', url: 'https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/' },
    ],
  },
  {
    day: 21,
    enabled: true,
    title: 'GitOps Drift',
    description: 'An ArgoCD Application can\'t find its manifests because the source path is wrong. Fix the Application to sync successfully.',
    flagHash: 'caef9c654e06a53987e52e1eba02a38ecb543cd10c88d27287460d6c470edd84',
    setup: [
      'Install ArgoCD:',
      '<code>kubectl create namespace argocd\nkubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml</code>',
      'Install the chart:',
      '<code>helm install day21 oci://ghcr.io/adventofkube/charts/day21 --version 0.1.0</code>',
      'Check the Application:',
      '<code>kubectl get application -n argocd</code>',
    ],
    hints: [
      'The Application shows a sync error. Check its status:<code>kubectl describe application gitops-app -n argocd</code>What path is it looking at?',
      'The source path is "deploy/manifests" which doesn\'t exist in the repo. Check the hint ConfigMap for clues.',
      'Fix the Application spec.source.path to point to the correct location in the repo.',
    ],
    docs: [
      { title: 'ArgoCD Getting Started', url: 'https://argo-cd.readthedocs.io/en/stable/getting_started/' },
      { title: 'ArgoCD Application', url: 'https://argo-cd.readthedocs.io/en/stable/operator-manual/declarative-setup/' },
    ],
  },
  {
    day: 22,
    enabled: true,
    title: 'Sync Hooks',
    description: 'An ArgoCD PreSync hook Job is failing, blocking the entire sync. The main app never deploys. Fix the hook to unblock.',
    flagHash: '601d72ebe7744ffc429562eb05f585a1b72f65ffef0c3d01cb836b25991221be',
    setup: [
      'Ensure ArgoCD is installed (see Day 21).',
      'Install the chart:',
      '<code>helm install day22 oci://ghcr.io/adventofkube/charts/day22 --version 0.1.0</code>',
      'Check the Job status:',
      '<code>kubectl get jobs -n day22</code>',
    ],
    hints: [
      'The PreSync Job is failing. Check the pod status:<code>kubectl describe job db-migrate -n day22</code>What\'s the error?',
      'The Job uses image busybox:99.99 which doesn\'t exist. It\'s stuck in ImagePullBackOff.',
      'Fix the Job image to a valid version (e.g., busybox:1.36). Delete the failed Job, then trigger a sync to retry.',
    ],
    docs: [
      { title: 'ArgoCD Resource Hooks', url: 'https://argo-cd.readthedocs.io/en/stable/user-guide/resource_hooks/' },
      { title: 'Jobs', url: 'https://kubernetes.io/docs/concepts/workloads/controllers/job/' },
    ],
  },
  {
    day: 23,
    enabled: true,
    title: 'Policy Blocked',
    description: 'A Kyverno policy is blocking pod creation because a required label is missing. Fix the Deployment to comply.',
    flagHash: 'e888e89997e46deb71c4a61205a2fa03dba6ccda40fd7ce6bf9a2848bf264165',
    setup: [
      'Install Kyverno:',
      '<code>helm repo add kyverno https://kyverno.github.io/kyverno\nhelm install kyverno kyverno/kyverno -n kyverno --create-namespace</code>',
      'Install the chart:',
      '<code>helm install day23 oci://ghcr.io/adventofkube/charts/day23 --version 0.1.0</code>',
      'Check pod creation:',
      '<code>kubectl get pods -n day23\nkubectl get events -n day23 --sort-by=.lastTimestamp</code>',
    ],
    hints: [
      'Pods aren\'t being created. Check the events for policy violations:<code>kubectl get events -n day23 | grep -i kyverno</code>',
      'A ClusterPolicy requires all pods in this namespace to have a "team" label. The Deployment only has "app".',
      'Add the team label to the pod template:<code>kubectl patch deployment policy-app -n day23 -p \'{"spec":{"template":{"metadata":{"labels":{"team":"platform"}}}}}\'</code>',
    ],
    docs: [
      { title: 'Kyverno Policies', url: 'https://kyverno.io/docs/writing-policies/' },
      { title: 'Kyverno Validate Rules', url: 'https://kyverno.io/docs/writing-policies/validate/' },
    ],
  },
  {
    day: 24,
    enabled: true,
    title: 'Webhook Woes',
    description: 'A ValidatingWebhook is rejecting your app\'s pods. Understand what the webhook requires and fix the Deployment to pass validation.',
    flagHash: '3f50ffc39ffdfc130953c5c70a31033ef4ecb34641758b03933f2931f5a758d7',
    setup: [
      'Install the chart:',
      '<code>helm install day24 oci://ghcr.io/adventofkube/charts/day24 --version 0.1.0</code>',
      'Wait for the webhook to be ready, then check pods:',
      '<code>kubectl get pods -n day24\nkubectl get events -n day24 --sort-by=.lastTimestamp</code>',
    ],
    hints: [
      'The app deployment\'s pods are being rejected. Check the events for admission errors:<code>kubectl get events -n day24 | grep -i rejected</code>What does the webhook say?',
      'The webhook rejects pods without annotation "approved: true". Check the webhook configuration:<code>kubectl get validatingwebhookconfiguration day24-approval-check -o yaml</code>',
      'Add the annotation to the Deployment pod template:<code>kubectl patch deployment webhook-app -n day24 -p \'{"spec":{"template":{"metadata":{"annotations":{"approved":"true"}}}}}\'</code>',
    ],
    docs: [
      { title: 'Dynamic Admission Control', url: 'https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/' },
      { title: 'ValidatingWebhookConfiguration', url: 'https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#webhook-configuration' },
    ],
  },
  {
    day: 25,
    enabled: true,
    title: 'Grand Finale',
    description: 'Three namespaces, seven bugs, one flag. RBAC is wrong, NetworkPolicies block everything, pods are misconfigured. Fix all three services and combine their fragments.',
    flagHash: '5f3ed1402387d339eabcea9fe4b8d8745e5b74db497ad1669e45bea59bc7df20',
    setup: [
      'Install the chart:',
      '<code>helm install day25 oci://ghcr.io/adventofkube/charts/day25 --version 0.1.0</code>',
      'Check all three namespaces:',
      '<code>kubectl get pods -n finale-a\nkubectl get pods -n finale-b\nkubectl get pods -n finale-c</code>',
      'Each service returns a flag fragment. Combine all three for the full flag.',
    ],
    hints: [
      'Start with each namespace separately. Use <code>kubectl describe</code> and <code>kubectl logs</code> to identify the issues in each.',
      'finale-a has an RBAC issue (check the Role verbs). finale-b has a NetworkPolicy + Service port mismatch. finale-c has a missing ConfigMap + wrong liveness path + OOMKilled.',
      'Fix each namespace\'s issues, curl each service, and concatenate the three fragments to get the complete flag.',
    ],
    docs: [
      { title: 'Debugging Pods', url: 'https://kubernetes.io/docs/tasks/debug/debug-application/debug-pods/' },
      { title: 'Debugging Services', url: 'https://kubernetes.io/docs/tasks/debug/debug-application/debug-service/' },
      { title: 'RBAC', url: 'https://kubernetes.io/docs/reference/access-authn-authz/rbac/' },
      { title: 'Network Policies', url: 'https://kubernetes.io/docs/concepts/services-networking/network-policies/' },
    ],
  },
];
